# Sample Configuration File for SFT Training Pipeline
# Copy this file and modify according to your needs

# Model configurations
model_name_or_path: "microsoft/DialoGPT-medium"  # HuggingFace model name or local path
tokenizer_name: null  # Use same as model if null
model_revision: "main"
use_auth_token: false

# Data configurations
dataset_name: null  # HuggingFace dataset name (e.g., "squad", "glue")
dataset_config_name: null  # Dataset configuration name
dataset_path: "./data"  # Local path for data files
train_file: "train.jsonl"  # Training file name
validation_file: "validation.jsonl"  # Validation file name
max_seq_length: 512
preprocessing_num_workers: 4

# Training configurations
output_dir: "./sft_output"
overwrite_output_dir: true
do_train: true
do_eval: true
per_device_train_batch_size: 4  # Batch size per GPU
per_device_eval_batch_size: 4
gradient_accumulation_steps: 1  # Effective batch size = per_device_batch_size * gradient_accumulation_steps * num_gpus
learning_rate: 5.0e-5
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1.0e-8
max_grad_norm: 1.0
num_train_epochs: 3
max_steps: -1  # Set to positive number to override num_train_epochs
warmup_steps: 500
lr_scheduler_type: "linear"  # "linear", "cosine", "cosine_with_restarts", "polynomial", "constant", "constant_with_warmup"

# Logging and evaluation
logging_steps: 10
save_steps: 500
eval_steps: 500
save_total_limit: 3
evaluation_strategy: "steps"  # "no", "steps", "epoch"
save_strategy: "steps"  # "no", "steps", "epoch"
load_best_model_at_end: true
metric_for_best_model: "eval_loss"
greater_is_better: false

# Distributed training configurations
num_gpus: 1  # Number of GPUs to use
ddp_backend: "nccl"  # "nccl", "gloo", "mpi"

# Optimization configurations
fp16: true  # Use mixed precision training
bf16: false  # Use bfloat16 (better than fp16 on newer hardware)
gradient_checkpointing: true  # Save memory at cost of computation
dataloader_num_workers: 4
dataloader_pin_memory: true

# Logging and monitoring
logging_dir: "./logs"
report_to: "wandb"  # "wandb", "tensorboard", "none"
run_name: "sft_training_run"
wandb_project: "sft-training"  # W&B project name

# Advanced configurations
resume_from_checkpoint: null  # Path to checkpoint directory
seed: 42

# Optional: LoRA (Low-Rank Adaptation) configuration
# Uncomment and modify if you want to use LoRA for parameter-efficient fine-tuning
# lora_config:
#   r: 16  # Rank of adaptation
#   lora_alpha: 32  # LoRA scaling parameter
#   target_modules: ["q_proj", "v_proj"]  # Modules to apply LoRA to
#   lora_dropout: 0.1
#   bias: "none"  # "none", "all", "lora_only"
#   task_type: "CAUSAL_LM"

# Optional: Quantization configuration
# Uncomment if you want to use quantization to save memory
# quantization_config:
#   load_in_4bit: true  # Use 4-bit quantization
#   bnb_4bit_quant_type: "nf4"  # Quantization type
#   bnb_4bit_compute_dtype: "float16"  # Compute dtype
#   bnb_4bit_use_double_quant: true  # Use double quantization